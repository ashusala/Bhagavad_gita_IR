{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import important libraries\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from gensim.models import Word2Vec \n",
    "from numpy.linalg import norm\n",
    "from numpy import dot\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = open('data_preprocessed.txt','r')\n",
    "s = sample.read()\n",
    "f = s.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for Word2Vec model\n",
    "data = []\n",
    "\n",
    "# iterate through each sentence in the file\n",
    "for i in sent_tokenize(f):\n",
    "    temp = []\n",
    "    \n",
    "    # tokenize the setence into words\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j)\n",
    "        \n",
    "    data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model = gensim.models.Word2Vec(data, min_count = 1, size = 100, window = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('fragment', 0.7956534624099731),\n",
       " ('motherthe', 0.7932881712913513),\n",
       " ('wasp', 0.7788851857185364),\n",
       " ('gust', 0.7763956785202026),\n",
       " ('sample', 0.7721961736679077),\n",
       " ('box', 0.7672709226608276),\n",
       " ('semblance', 0.7669326066970825),\n",
       " ('incompatibility', 0.765509843826294),\n",
       " ('glimpse', 0.764214038848877),\n",
       " ('reverend', 0.7615311741828918)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_word('consequence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get the word frequency of each word in a verse/sentence\n",
    "# list_of_words: a pre-processed list of words in a verse/sentence.\n",
    "def word_count(list_of_words):\n",
    "    \n",
    "    #dictionary keeping frequency for all the unique words\n",
    "    counts = dict()\n",
    "    \n",
    "    for word in list_of_words:\n",
    "        #if word already listed, increase its count\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        # else putting the new word in dictionary\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate composite sentence/verse embeddings\n",
    "# verse to be given in tokenzied and preprocessed format to func.\n",
    "def senemb(alpha, verse_tok, model, dim):\n",
    "    # create an empty vector\n",
    "    sentEmb = np.zeros(dim)\n",
    "    v_count = word_count(verse_tok)\n",
    "    \n",
    "    for word in verse_tok:\n",
    "        prob = alpha / (alpha + v_count[word]/len(verse))\n",
    "        sentEmb = sentEmb +  model.wv[word]  * prob\n",
    "        \n",
    "    return sentEmb/len(verse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "29\n",
      "23\n",
      "22\n",
      "15\n",
      "25\n",
      "25\n",
      "23\n",
      "34\n",
      "24\n",
      "27\n",
      "32\n",
      "19\n",
      "22\n",
      "28\n",
      "57\n",
      "26\n",
      "49\n",
      "37\n",
      "18\n",
      "28\n",
      "25\n",
      "29\n",
      "23\n",
      "31\n",
      "24\n",
      "31\n",
      "30\n",
      "109\n",
      "48\n",
      "39\n",
      "23\n",
      "29\n",
      "41\n",
      "29\n",
      "23\n",
      "30\n",
      "21\n",
      "26\n",
      "21\n",
      "39\n",
      "30\n",
      "29\n",
      "47\n",
      "37\n",
      "47\n",
      "41\n",
      "19\n",
      "24\n",
      "33\n",
      "27\n",
      "35\n",
      "47\n",
      "25\n",
      "40\n",
      "21\n",
      "24\n",
      "26\n",
      "45\n",
      "24\n",
      "25\n",
      "24\n",
      "25\n",
      "20\n",
      "28\n",
      "32\n",
      "25\n",
      "32\n",
      "23\n",
      "31\n",
      "22\n",
      "29\n",
      "17\n",
      "30\n",
      "20\n",
      "33\n",
      "28\n",
      "44\n",
      "26\n",
      "30\n",
      "49\n",
      "36\n",
      "41\n",
      "36\n",
      "39\n",
      "18\n",
      "31\n",
      "29\n",
      "49\n",
      "29\n",
      "32\n",
      "35\n",
      "47\n",
      "34\n",
      "28\n",
      "26\n",
      "30\n",
      "28\n",
      "24\n",
      "24\n",
      "31\n",
      "28\n",
      "26\n",
      "37\n",
      "28\n",
      "16\n",
      "28\n",
      "42\n",
      "36\n",
      "37\n",
      "27\n",
      "20\n",
      "41\n",
      "20\n",
      "32\n",
      "22\n",
      "30\n",
      "22\n",
      "42\n",
      "49\n",
      "23\n",
      "42\n",
      "34\n",
      "27\n",
      "30\n",
      "41\n",
      "32\n",
      "37\n",
      "26\n",
      "28\n",
      "21\n",
      "40\n",
      "21\n",
      "35\n",
      "28\n",
      "52\n",
      "29\n",
      "34\n",
      "37\n",
      "28\n",
      "26\n",
      "31\n",
      "27\n",
      "37\n",
      "39\n",
      "21\n",
      "37\n",
      "34\n",
      "27\n",
      "28\n",
      "30\n",
      "31\n",
      "39\n",
      "37\n",
      "41\n",
      "37\n",
      "26\n",
      "29\n",
      "31\n",
      "26\n",
      "24\n",
      "32\n",
      "38\n",
      "20\n",
      "27\n",
      "44\n",
      "34\n",
      "25\n",
      "32\n",
      "25\n",
      "28\n",
      "43\n",
      "25\n",
      "39\n",
      "35\n",
      "26\n",
      "41\n",
      "23\n",
      "34\n",
      "35\n",
      "34\n",
      "58\n",
      "31\n",
      "25\n",
      "30\n",
      "33\n",
      "34\n",
      "41\n",
      "34\n",
      "24\n",
      "44\n",
      "31\n",
      "33\n",
      "41\n",
      "28\n",
      "35\n",
      "32\n",
      "35\n",
      "42\n",
      "39\n",
      "33\n",
      "36\n",
      "61\n",
      "27\n",
      "20\n",
      "39\n",
      "33\n",
      "39\n",
      "25\n",
      "27\n",
      "32\n",
      "27\n",
      "31\n",
      "33\n",
      "35\n",
      "43\n",
      "36\n",
      "33\n",
      "39\n",
      "31\n",
      "55\n",
      "44\n",
      "46\n",
      "38\n",
      "38\n",
      "26\n",
      "28\n",
      "30\n",
      "34\n",
      "51\n",
      "32\n",
      "40\n",
      "77\n",
      "52\n",
      "32\n",
      "28\n",
      "23\n",
      "30\n",
      "27\n",
      "108\n",
      "44\n",
      "35\n",
      "27\n",
      "42\n",
      "30\n",
      "26\n",
      "22\n",
      "27\n",
      "28\n",
      "25\n",
      "24\n",
      "29\n",
      "30\n",
      "39\n",
      "36\n",
      "28\n",
      "40\n",
      "32\n",
      "32\n",
      "32\n",
      "35\n",
      "33\n",
      "26\n",
      "46\n",
      "35\n",
      "25\n",
      "24\n",
      "19\n",
      "27\n",
      "35\n",
      "22\n",
      "35\n",
      "31\n",
      "26\n",
      "29\n",
      "45\n",
      "23\n",
      "29\n",
      "31\n",
      "35\n",
      "34\n",
      "43\n",
      "34\n",
      "26\n",
      "32\n",
      "26\n",
      "34\n",
      "43\n",
      "30\n",
      "43\n",
      "24\n",
      "35\n",
      "31\n",
      "46\n",
      "34\n",
      "31\n",
      "42\n",
      "63\n",
      "25\n",
      "22\n",
      "45\n",
      "30\n",
      "64\n",
      "43\n",
      "43\n",
      "40\n",
      "38\n",
      "25\n",
      "29\n",
      "39\n",
      "24\n",
      "28\n",
      "24\n",
      "38\n",
      "32\n",
      "31\n",
      "29\n",
      "51\n",
      "38\n",
      "38\n",
      "19\n",
      "51\n",
      "39\n",
      "43\n",
      "31\n",
      "22\n",
      "47\n",
      "19\n",
      "31\n",
      "27\n",
      "22\n",
      "36\n",
      "25\n",
      "31\n",
      "40\n",
      "20\n",
      "29\n",
      "32\n",
      "37\n",
      "37\n",
      "29\n",
      "36\n",
      "46\n",
      "27\n",
      "29\n",
      "22\n",
      "41\n",
      "20\n",
      "28\n",
      "36\n",
      "37\n",
      "29\n",
      "20\n",
      "27\n",
      "30\n",
      "29\n",
      "37\n",
      "27\n",
      "28\n",
      "44\n",
      "37\n",
      "24\n",
      "32\n",
      "33\n",
      "22\n",
      "22\n",
      "53\n",
      "25\n",
      "27\n",
      "16\n",
      "32\n",
      "37\n",
      "29\n",
      "26\n",
      "27\n",
      "35\n",
      "33\n",
      "25\n",
      "33\n",
      "33\n",
      "29\n",
      "29\n",
      "35\n",
      "26\n",
      "28\n",
      "35\n",
      "27\n",
      "30\n",
      "31\n",
      "29\n",
      "28\n",
      "32\n",
      "23\n",
      "28\n",
      "16\n",
      "24\n",
      "24\n",
      "25\n",
      "44\n",
      "29\n",
      "26\n",
      "35\n",
      "47\n",
      "20\n",
      "24\n",
      "48\n",
      "28\n",
      "29\n",
      "27\n",
      "40\n",
      "35\n",
      "39\n",
      "40\n",
      "42\n",
      "31\n",
      "44\n",
      "37\n",
      "39\n",
      "38\n",
      "35\n",
      "36\n",
      "22\n",
      "19\n",
      "29\n",
      "45\n",
      "41\n",
      "38\n",
      "33\n",
      "39\n",
      "46\n",
      "48\n",
      "50\n",
      "48\n",
      "30\n",
      "73\n",
      "55\n",
      "59\n",
      "51\n",
      "37\n",
      "42\n",
      "50\n",
      "38\n",
      "32\n",
      "34\n",
      "38\n",
      "43\n",
      "37\n",
      "48\n",
      "28\n",
      "35\n",
      "53\n",
      "32\n",
      "52\n",
      "28\n",
      "34\n",
      "28\n",
      "28\n",
      "43\n",
      "65\n",
      "31\n",
      "33\n",
      "27\n",
      "63\n",
      "27\n",
      "62\n",
      "31\n",
      "38\n",
      "36\n",
      "45\n",
      "102\n",
      "31\n",
      "24\n",
      "48\n",
      "44\n",
      "39\n",
      "42\n",
      "30\n",
      "24\n",
      "29\n",
      "38\n",
      "31\n",
      "36\n",
      "22\n",
      "40\n",
      "33\n",
      "31\n",
      "25\n",
      "27\n",
      "28\n",
      "37\n",
      "34\n",
      "27\n",
      "36\n",
      "30\n",
      "34\n",
      "31\n",
      "29\n",
      "29\n",
      "36\n",
      "29\n",
      "36\n",
      "27\n",
      "45\n",
      "21\n",
      "30\n",
      "21\n",
      "19\n",
      "33\n",
      "39\n",
      "26\n",
      "39\n",
      "37\n",
      "36\n",
      "32\n",
      "132\n",
      "27\n",
      "23\n",
      "43\n",
      "43\n",
      "79\n",
      "47\n",
      "28\n",
      "29\n",
      "37\n",
      "36\n",
      "43\n",
      "38\n",
      "31\n",
      "27\n",
      "31\n",
      "38\n",
      "31\n",
      "26\n",
      "32\n",
      "30\n",
      "32\n",
      "77\n",
      "19\n",
      "29\n",
      "40\n",
      "30\n",
      "29\n",
      "25\n",
      "29\n",
      "49\n",
      "110\n",
      "25\n",
      "24\n",
      "38\n",
      "27\n",
      "30\n",
      "28\n",
      "26\n",
      "21\n",
      "33\n",
      "34\n",
      "35\n",
      "39\n",
      "29\n",
      "51\n",
      "34\n",
      "35\n",
      "29\n",
      "33\n",
      "28\n",
      "29\n",
      "38\n",
      "30\n",
      "24\n",
      "16\n",
      "27\n",
      "30\n",
      "22\n",
      "29\n",
      "31\n",
      "29\n",
      "43\n",
      "22\n",
      "31\n",
      "52\n",
      "32\n",
      "32\n",
      "44\n",
      "31\n",
      "26\n",
      "26\n",
      "31\n",
      "27\n",
      "34\n",
      "37\n",
      "24\n",
      "28\n",
      "36\n",
      "23\n",
      "28\n",
      "19\n",
      "26\n",
      "30\n",
      "28\n",
      "26\n",
      "31\n",
      "29\n",
      "37\n",
      "30\n",
      "29\n",
      "33\n",
      "39\n",
      "39\n",
      "43\n",
      "29\n",
      "47\n",
      "33\n",
      "35\n",
      "36\n",
      "26\n",
      "27\n",
      "34\n",
      "34\n",
      "34\n",
      "32\n",
      "27\n",
      "26\n",
      "20\n",
      "19\n",
      "23\n",
      "21\n",
      "26\n",
      "37\n",
      "38\n",
      "26\n",
      "36\n",
      "84\n",
      "42\n",
      "37\n",
      "22\n",
      "22\n",
      "40\n",
      "30\n",
      "32\n",
      "31\n",
      "22\n",
      "22\n",
      "31\n",
      "34\n",
      "21\n",
      "28\n",
      "25\n",
      "21\n",
      "17\n",
      "24\n",
      "24\n",
      "38\n",
      "28\n",
      "26\n",
      "23\n",
      "25\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "#sentence embedding for verses\n",
    "\n",
    "verses = pd.read_csv('verses.csv', usecols=['#verse', 'Translation'])\n",
    "data = verses['Translation'].to_list()\n",
    "\n",
    "#list to store all the vectors\n",
    "verse_vectors = []\n",
    "\n",
    "# loop over all the verses\n",
    "for verse in data:\n",
    "    # tokenize and preprocess the verse\n",
    "    verse_tok = word_tokenize(verse.lower().translate(str.maketrans('','',string.punctuation)))\n",
    "    verse_vectors.append(senemb(1, verse_tok, model, 100))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we calculate the sentence embedding vectors for queries\n",
    "\n",
    "# getting the queries\n",
    "df = pd.read_csv('queries_sem.csv')\n",
    "queries_ = df['Verse'].to_list()\n",
    "\n",
    "# pre-processing the queries\n",
    "queries = [query.lower().translate(str.maketrans('','',string.punctuation)).split() for query in queries_]\n",
    "\n",
    "# a list to store all the vectors\n",
    "query_vectors = []\n",
    "\n",
    "for query in queries:\n",
    "    query_vectors.append(senemb(1, query, model, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' can I get rid of my sins?'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries_scores: to store cos. sim. of each query with all the verses. [basically a 2d list]\n",
    "queries_scores = []\n",
    "\n",
    "# looping over all the queries\n",
    "for q_vec in query_vectors:\n",
    "    query_scores = []\n",
    "    # storing the scores for a particular query\n",
    "    for v_vec in verse_vectors:\n",
    "        # getting the score\n",
    "        score = (dot(q_vec , v_vec))/(norm(q_vec)*norm(v_vec))\n",
    "        query_scores.append(score)\n",
    "        \n",
    "    queries_scores.append(query_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's move towars getting the answers for the queries\n",
    "f = open('best3_word2vec.txt','w+')\n",
    "\n",
    "# top3_verseNo: to store verse nos. of top three verses for each query\n",
    "# to be put as results in mAP fun.\n",
    "top3_verseNo = []\n",
    "\n",
    "# looping over \n",
    "for ind, i in enumerate(queries_scores):\n",
    "    temp = [] # to store the top 3 verses \n",
    "    indices = [] # to store the top 3 indices\n",
    "    \n",
    "    #take three elements with highest scores   \n",
    "    for ele in sorted(i)[-1:-4:-1]:\n",
    "        #top_result = verses.iloc[i.index(ele)]\n",
    "        indices.append(i.index(ele))\n",
    "        temp.append(verses.iloc[i.index(ele)]['#verse'].replace(\":\",\"\"))\n",
    "        \n",
    "    # printing the results in a file\n",
    "    f.write('For query:\\n' + '\"\"' + queries_[ind] + '\"\"' + '\\n\\n')\n",
    "    f.write('The Best 3 matching verses are:\\n')\n",
    "    for ind, i in enumerate(indices):\n",
    "        f.write(str(ind + 1) + ') ' + verses.iloc[i]['#verse'] + verses.iloc[i]['Translation'] + '\\n\\n')\n",
    "\n",
    "    top3_verseNo.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  #verse                                        Translation\n",
      "1   1.2:  Sanjaya said: O King, after looking over the a...\n",
      "2   1.3:  O my teacher, behold the great army of the son...\n"
     ]
    }
   ],
   "source": [
    "print(str(verses.iloc[[1,2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
